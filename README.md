# ğŸ“° PubMed Paper Fetcher

**Find industry-authored biomedical papers in seconds**  
A fast, scriptable CLI that queries PubMed, detects papers with at least one **pharma / biotech author**, and exports the results as a tidy CSV.

---

## ğŸš€  Key Features

| Capability | Details |
|------------|---------|
| ğŸ” **Flexible Query** | Accepts *any* PubMed query syntax (Boolean, MeSH, date ranges, etc.). |
| ğŸ¢ **Industry Detection** | Flags authors whose affiliations look like companies (Inc, Ltd, Pharma, Biotech â€¦) and ignores purely academic addresses. |
| ğŸ“‘ **Rich CSV Output** | `PubmedID Â· Title Â· Publication Date Â· Non-academic Author(s) Â· Company Affiliation(s) Â· Corresponding Email` |
| ğŸ’¬ **Verbose / Quiet Modes** | `-d` flag prints every API call and parsing step; default keeps output clean. |
| ğŸ’¾ **Export or Pipe** | Print to console *or* save directly to a file with `-f results.csv`. |
| âš¡ **Progress Bar** | Live download progress via Rich. |
| â±ï¸ **Fast, Parallel Batching** | Fetches up to 200 articles per API call (polite to NCBI). |
| ğŸ›  **Extensible Filters** | Company/academic keyword lists are centralisedâ€”swap in your own domain rules easily. |

---

## ğŸ›   Setup Instructions

### 1  Clone & create a virtual environment

```bash
git clone https://github.com/<YOUR_USER>/pubmed-paper-fetcher.git
cd pubmed-paper-fetcher

python -m venv .venv                      # optional but recommended
# Windows PowerShell
.\.venv\Scripts\Activate.ps1
# macOS / Linux
# source .venv/bin/activate

```

### 2 Install dependencies

```bash
python -m pip install -r requirements.txt
# (Optional) editable install so CLI works everywhere in the venv
python -m pip install -e .

```
### 3 Run the CLI
```bash
# Print to console
python -m pubmed_paper_fetcher "diabetes AND insulin"

# Save top 500 hits to CSV with debug logs
python -m pubmed_paper_fetcher -l 500 -d -f diabetes_industry.csv "diabetes AND insulin"

```
### 4 Architecture & Reasoning Flow
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   Query â”€â”€â”€â–º â”‚   ESearch     â”‚â”€â”     (returns PubMed IDs)
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
                                â–¼
                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   batched 200 IDs
                          â”‚   EFetch    â”‚â”€â”€â”€â”€â”€â”€â”
                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
                                               â–¼
                                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                   â”‚   XML  Parser      â”‚
                                   â”‚  â€¢ extract title   â”‚
                                   â”‚  â€¢ date            â”‚
                                   â”‚  â€¢ authors + affil â”‚
                                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                               â”‚
                         regex filter          â–¼
                (company vs academic)  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                        â”‚   Row Builder   â”‚
                                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                               â”‚
                                          pandas DF
                                               â”‚
                                     CSV file  â–½  console
                                     
1. ESearch â€“ Retrieves up to N PubMed IDs for the user query

2. EFetch â€“ Downloads full XML metadata in batches (â‰¤ 300 IDs/request)

3. XML Parser â€“ Extracts key fields & affiliation text

4. Industry Filter â€“ Regex rules flag non-academic authors

5. Output Layer â€“ Builds a DataFrame â†’ saves to CSV or prints

Extending / Customising
Company keyword list â†’ edit the COMPANY_KEYWORDS regex in fetcher.py.

Academic exclusions â†’ tweak ACADEMIC_KEYWORDS.

JSON output â€“ add a --json flag and call df.to_json().

Higher concurrency â€“ wrap efetch calls with asyncio or ThreadPoolExecutor (respect NCBI rate limits!).
